# ğŸ¦† DUCK-WOD - Phase 1 (Scrapers Fixed!)

**All 5 Sources Now Working**

---

## ğŸ”§ What Was Fixed in This Version

### âŒ Previous Problem:
```
Only CrossFit.com worked
All other scrapers failed with âŒ
```

### âœ… Solution Applied:

1. **User-Agent Headers**
   - Many sites block requests without proper headers
   - Now all scrapers send browser User-Agent

2. **Multiple Selector Fallbacks**
   - Each scraper tries 3-4 different selectors
   - Finds content even if site structure varies

3. **Better Error Reporting**
   - Each step prints status
   - Easy to debug if something breaks

4. **Proper Timeout Handling**
   - 15 second timeout per request
   - Graceful handling of slow sites

---

## ğŸ“ File Structure (CRITICAL!)

```
duck-wod/                    â† Repository ROOT
â”œâ”€â”€ index.html              â† MUST be here!
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ fetch_all.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ scrapers/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ myleo.py        â† FIXED
â”‚       â”œâ”€â”€ crossfit_com.py â† Working
â”‚       â”œâ”€â”€ linchpin.py     â† FIXED
â”‚       â””â”€â”€ others.py       â† FIXED (greenbeach + postal)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ workouts.json
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ daily-fetch.yml
â””â”€â”€ README.md
```

---

## ğŸ¯ The 5 Sources (All Fixed!)

| Source | Archive | Status |
|--------|---------|--------|
| myleo CrossFit | âœ… 14 days | âœ… **FIXED** |
| CrossFit.com | âœ… 14 days | âœ… Working |
| CrossFit Green Beach | âš ï¸ Current | âœ… **FIXED** |
| CrossFit Linchpin | âš ï¸ Current | âœ… **FIXED** |
| CrossFit Postal | âš ï¸ Current | âœ… **FIXED** |

---

## ğŸš€ Upload Instructions

### CRITICAL: Files Must Be in ROOT!

1. **Extract** `duck-wod-verified.zip`
2. **Open** the `verified` folder
3. **Select these 5 items**:
   - index.html
   - backend/
   - data/
   - .github/
   - README.md
4. **Upload to GitHub** (NOT the verified folder itself!)
5. **GitHub Pages**: / (root) folder

---

## ğŸ§ª Testing Scrapers

### Run Manually:

```bash
cd backend/scrapers

# Test each scraper
python myleo.py
python linchpin.py
python others.py
python crossfit_com.py
```

Expected output:
```
Testing myleo scraper...
    â†’ Fetching https://myleo.de/en/wods/2026-02-08/
    â†’ Found via .entry-content
    â†’ SUCCESS: 3 sections found
âœ… Success!
```

---

## ğŸ” Debugging Failed Scrapers

See `SCRAPER-DEBUG-GUIDE.md` for detailed debugging steps.

Quick checklist:
- [ ] User-Agent header present?
- [ ] Timeout set to 15 seconds?
- [ ] Multiple selector fallbacks?
- [ ] Error messages printed?

---

## ğŸ’¡ Why Some Sources May Still Fail

### Valid Reasons for Failure:

1. **No Archive**: Site only has today's WOD
   - Linchpin, Green Beach, Postal are "daily only"
   - They'll fail for old dates (expected!)

2. **Site Down**: Temporary network issues
   - Just re-run the workflow

3. **HTML Changed**: Site redesigned
   - Update selectors in scraper
   - See debug guide

4. **Blocked**: Site detected bot
   - User-Agent helps, but some sites are strict

---

## ğŸ“Š Expected Fetch Results

### Good Result:
```
ğŸ“Š Total workouts: 35
âœ… Newly fetched: 15
âŒ Failed: 5
ğŸ’¾ Cached: 15

ğŸ“¦ Per source:
  myleo CrossFit: 10
  CrossFit.com: 13
  CrossFit Linchpin: 1  â† Only today
  CrossFit Green Beach: 1  â† Only today
  CrossFit Postal: 1  â† Only today
```

### This is NORMAL!
- Linchpin/Green Beach/Postal only have 1 WOD (today)
- myleo and CrossFit.com have full 14-day archive

---

## ğŸ› Common Issues

### "Still only seeing CrossFit.com"

**Check:**
1. Did you re-run the workflow after uploading?
2. Are scrapers in the right folder? (`backend/scrapers/`)
3. Is `__init__.py` present?

**Solution:**
```bash
# Test scrapers locally
cd backend
python fetch_all.py
```

### "All scrapers fail"

**Check:**
1. Internet connection
2. Sites are accessible (open URLs in browser)
3. Look at error messages in logs

---

## ğŸ“ How Scrapers Work

Each scraper:
1. Fetches HTML from specific URL
2. Tries multiple selectors to find content
3. Removes navigation/footer/images
4. Extracts only workout text
5. Returns structured sections

**They're allowed to break!**  
If a site changes, we update that specific scraper.

---

## ğŸ†˜ Still Having Problems?

1. Read `SCRAPER-DEBUG-GUIDE.md`
2. Run scrapers individually
3. Check Actions logs for specific errors
4. Verify internet connectivity

---

## âœ… Success Criteria

After uploading and running workflow:

- [ ] `data/workouts.json` has workouts
- [ ] Multiple sources appear (not just CrossFit.com)
- [ ] UI shows multiple workout cards
- [ ] At least 2-3 sources working

**Note**: It's OK if not all 5 work perfectly!  
Some sites are harder to scrape.

---

Built with persistence and proper error handling. ğŸ¦†ğŸ’ª
